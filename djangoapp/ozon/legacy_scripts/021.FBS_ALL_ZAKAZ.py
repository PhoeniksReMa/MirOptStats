# ====== –°–º–µ–Ω–∞ CWD –Ω–∞ iPhone –î–û –ª—é–±—ã—Ö –∏–º–ø–æ—Ä—Ç–æ–≤ ======
import os, sys, tempfile
if sys.platform == "ios":  # Pyto (iOS)
    SAFE_DIR = os.path.expanduser("~/Documents")
    try:
        os.makedirs(SAFE_DIR, exist_ok=True)
    except Exception:
        pass
    try:
        os.chdir(SAFE_DIR)
    except Exception:
        os.chdir(tempfile.gettempdir())
else:
    SAFE_DIR = os.path.expanduser("~/Documents")  # –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –Ω–∞ –ü–ö

# ====== –û—Å–Ω–æ–≤–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã ======
import re
import time
import json
import logging
from datetime import datetime, timedelta, timezone
from typing import Dict, Iterable, List, Optional
from zoneinfo import ZoneInfo

import requests
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# –ü–∞–ø–∫–∞ —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º (–Ω–∞ –ü–ö –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è)
try:
    SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
except NameError:
    SCRIPT_DIR = os.getcwd()

# ====== iCloud-aware –≤—ã–±–æ—Ä –ø–∞–ø–∫–∏ –≤—ã–≤–æ–¥–∞ –Ω–∞ iOS (–æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤) ======
def _looks_like_icloud_path(path: str) -> bool:
    low = path or ""
    return ("Mobile Documents" in low) or ("iCloud~" in low)

def _pyto_icloud_docs() -> str:
    return os.path.expanduser("~/Library/Mobile Documents/iCloud~org.python.pyto/Documents")

if sys.platform == "ios":
    if _looks_like_icloud_path(SCRIPT_DIR):
        OUTPUT_DIR = SCRIPT_DIR
    else:
        icloud_docs = _pyto_icloud_docs()
        OUTPUT_DIR = icloud_docs if os.path.isdir(icloud_docs) else SAFE_DIR
else:
    OUTPUT_DIR = SCRIPT_DIR

try:
    os.makedirs(OUTPUT_DIR, exist_ok=True)
except Exception:
    pass

# ====== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ======
DAYS_BACK = 5  # –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω–æ –æ—Ç 00:00 –ø–æ –ú–°–ö (—Å–º. iter_ozon_postings_last_ndays)

# –õ–∏—Å—Ç ‚Äî –∂–µ—Å—Ç–∫–æ –∑–∞—à–∏—Ç
SHEET_NAME = "üü¶–ó–∞–∫–∞–∑—ã_FBSüöõ"

START_ROW = 5
HEADER_ROW = 4
LIMIT = 1000
TIMEOUT = 30
MAX_RETRIES = 5
BACKOFF_BASE = 0.5

logging.basicConfig(level=logging.INFO, format="%(message)s")
log = logging.getLogger("ozon-sync")

# ====== –ß–∏—Ç–∞–µ–º Client ID / Api-key / Spreadsheet ID –∏–∑ API.txt ======
def _load_api_txt() -> tuple[str, str, str]:
    """
    API.txt –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, 3 —Å—Ç—Ä–æ–∫–∏:
    1) Client ID
    2) Api-key
    3) ID —Ç–∞–±–ª–∏—Ü—ã (SPREADSHEET_ID)
    """
    candidates = [
        os.path.join(SCRIPT_DIR, "API.txt"),
        os.path.join(OUTPUT_DIR, "API.txt"),
        os.path.join(SAFE_DIR, "API.txt"),
    ]
    api_path = next((p for p in candidates if os.path.isfile(p)), None)
    if not api_path:
        raise FileNotFoundError(
            "–ù–µ –Ω–∞–π–¥–µ–Ω API.txt.\n"
            "–ü–æ–ª–æ–∂–∏ API.txt —Ä—è–¥–æ–º —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º (–∏–ª–∏ –≤ OUTPUT_DIR / Documents).\n\n"
            "–§–æ—Ä–º–∞—Ç (3 —Å—Ç—Ä–æ–∫–∏):\n"
            "1) Client ID\n2) Api-key\n3) Spreadsheet ID"
        )

    with open(api_path, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]

    if len(lines) < 3:
        raise ValueError(
            "–í API.txt –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –º–∏–Ω–∏–º—É–º 3 –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏:\n"
            "1) Client ID\n2) Api-key\n3) Spreadsheet ID"
        )

    return lines[0], lines[1], lines[2]

CLIENT_ID, API_KEY, SPREADSHEET_ID = _load_api_txt()

# ====== Google –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è: –¢–û–õ–¨–ö–û –∏–∑ —Ñ–∞–π–ª–∞ credentials.json ======
def _find_credentials_json() -> str:
    candidates = [
        os.path.join(SCRIPT_DIR, "credentials.json"),
        os.path.join(OUTPUT_DIR, "credentials.json"),
        os.path.join(SAFE_DIR, "credentials.json"),
    ]
    path = next((p for p in candidates if os.path.isfile(p)), None)
    if not path:
        raise FileNotFoundError(
            "–ù–µ –Ω–∞–π–¥–µ–Ω credentials.json.\n"
            "–ü–æ–ª–æ–∂–∏ credentials.json —Ä—è–¥–æ–º —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º (–∏–ª–∏ –≤ OUTPUT_DIR / Documents)."
        )
    return path

def get_gspread_client():
    scope = ["https://www.googleapis.com/auth/spreadsheets", "https://www.googleapis.com/auth/drive"]
    cred_path = _find_credentials_json()
    creds = ServiceAccountCredentials.from_json_keyfile_name(cred_path, scope)
    return gspread.authorize(creds)

# ====== –î–∞–Ω–Ω—ã–µ/–∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã ======
HEADER_TRANSLATIONS = {
    'Posting Number': '–ù–æ–º–µ—Ä –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏—è',
    'Status': '–°—Ç–∞—Ç—É—Å',
    'Offer ID': '–ê—Ä—Ç–∏–∫—É–ª',
    'Product Quantity': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ',
    'In Process At': '–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è',
    'Shipment Date': '–î–∞—Ç–∞ –æ—Ç–≥—Ä—É–∑–∫–∏',
    'Product Name': '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ',
    'Cluster To': '–ö–ª–∞—Å—Ç–µ—Ä –æ—Ç–ø—Ä–∞–≤–∫–∏',
    'Fin Price': '–¶–µ–Ω–∞',
    'Fin Actions': '–ê–∫—Ü–∏–∏',
}
FULL_HEADERS_EXTENDED = (
    'Posting Number','Status','Offer ID','Product Quantity',
    'In Process At','Shipment Date','Product Name','Cluster To',
    'Fin Price','Fin Actions',
)

# –í–ê–ñ–ù–û: –¥–ª—è FBS LIST –≤–∞–ª–∏–¥–µ–Ω awaiting_deliver, –∞ awaiting_delivery ‚Äî –ù–ï–¢.
STATUS_MAP = {
    'awaiting_packaging': '–û–∂–∏–¥–∞–µ—Ç —É–ø–∞–∫–æ–≤–∫–∏',
    'awaiting_deliver': '–û–∂–∏–¥–∞–µ—Ç –æ—Ç–≥—Ä—É–∑–∫–∏',
    'cancelled': '–û—Ç–º–µ–Ω–µ–Ω',
}

MONTHS_RU = {1:"–Ø–Ω–≤",2:"–§–µ–≤",3:"–ú–∞—Ä",4:"–ê–ø—Ä",5:"–ú–∞–π",6:"–ò—é–Ω",7:"–ò—é–ª",8:"–ê–≤–≥",9:"–°–µ–Ω",10:"–û–∫—Ç",11:"–ù–æ—è",12:"–î–µ–∫"}
TZ_MSK = ZoneInfo("Europe/Moscow")

def clean_value(v):
    if v is None:
        return ''
    if isinstance(v, (dict, list)):
        return json.dumps(v, ensure_ascii=False)
    if isinstance(v, str):
        return STATUS_MAP.get(v, v)
    return str(v)

def iso_to_moscow_str(dt_str: Optional[str]) -> str:
    if not dt_str:
        return ''
    try:
        if isinstance(dt_str, str) and dt_str.endswith('Z'):
            dt = datetime.fromisoformat(dt_str.replace('Z', '+00:00'))
        else:
            dt = datetime.fromisoformat(dt_str)
        dt = dt.astimezone(TZ_MSK)
        return f"{dt.day:02d} {MONTHS_RU[dt.month]} {dt:%H:%M}"
    except Exception:
        return clean_value(dt_str)

def make_final_headers() -> List[str]:
    return [HEADER_TRANSLATIONS.get(h, h) for h in FULL_HEADERS_EXTENDED]

# ====== Ozon API ======
class OzonClient:
    BASE_URL = "https://api-seller.ozon.ru"

    def __init__(self, client_id: str, api_key: str, timeout: int = TIMEOUT):
        self.s = requests.Session()
        self.s.headers.update({
            "Client-Id": client_id,
            "Api-Key": api_key,
            "Content-Type": "application/json"
        })
        self.timeout = timeout

    def post(self, path: str, payload: dict) -> dict:
        url = f"{self.BASE_URL}{path}"
        last_exc = None

        for attempt in range(MAX_RETRIES):
            try:
                r = self.s.post(url, json=payload, timeout=self.timeout)

                # 4xx ‚Äî –Ω–µ—Ç —Å–º—ã—Å–ª–∞ —Ä–µ—Ç—Ä–∞–∏—Ç—å, –ø–æ–∫–∞–∂–µ–º —Ç–µ–ª–æ –æ—Ç–≤–µ—Ç–∞
                if 400 <= r.status_code < 500 and r.status_code != 429:
                    raise requests.HTTPError(
                        f"{r.status_code} {r.reason}. Response: {r.text}"
                    )

                if r.status_code in (429, 500, 502, 503, 504):
                    raise requests.HTTPError(f"{r.status_code} {r.text}")

                r.raise_for_status()
                return r.json()

            except Exception as e:
                last_exc = e
                msg = str(e)

                # –µ—Å–ª–∏ —ç—Ç–æ 4xx (–∫—Ä–æ–º–µ 429) ‚Äî –Ω–µ —Ä–µ—Ç—Ä–∞–∏–º
                if (" 400 " in msg) or (" 401 " in msg) or (" 403 " in msg) or (" 404 " in msg):
                    break

                time.sleep(min(30, (2 ** attempt) * BACKOFF_BASE) + 0.1 * attempt)

        raise RuntimeError(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ API Ozon: {last_exc}")

def _fmt_ozon_utc(dt: datetime) -> str:
    # RFC3339 + –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã + Z
    return dt.astimezone(timezone.utc).isoformat(timespec="milliseconds").replace("+00:00", "Z")

def iter_ozon_postings_last_ndays(
    ozon: OzonClient,
    days_back: int,
    status: str = "awaiting_packaging"
) -> Iterable[dict]:
    """
    "days_back" –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω–æ –æ—Ç 00:00 –ø–æ –ú–°–ö, —á—Ç–æ–±—ã –∑–∞–∫–∞–∑—ã –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ –Ω–µ –≤—ã–ø–∞–¥–∞–ª–∏.
    """
    now_msk = datetime.now(TZ_MSK)

    # —Å 00:00 –ú–°–ö days_back –¥–Ω–µ–π –Ω–∞–∑–∞–¥
    since_msk = (now_msk - timedelta(days=days_back)).replace(hour=0, minute=0, second=0, microsecond=0)

    since_dt = since_msk.astimezone(timezone.utc)
    to_dt = now_msk.astimezone(timezone.utc)

    offset = 0
    while True:
        payload = {
            "dir": "ASC",
            "filter": {
                "since": _fmt_ozon_utc(since_dt),
                "to": _fmt_ozon_utc(to_dt),
                "status": status
            },
            "limit": LIMIT,
            "offset": offset,
            "translit": True,
            "with": {"analytics_data": True, "financial_data": True}
        }

        data = ozon.post("/v3/posting/fbs/list", payload)
        res = data.get("result") or {}
        posts = res.get("postings") or []
        for p in posts:
            yield p

        if not res.get("has_next"):
            break
        offset += LIMIT

# ‚úÖ –ø–æ–ª—É—á–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç–∞—Ç—É—Å–æ–≤ –∏ –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º –ø–æ posting_number
def fetch_postings_multi_status(
    ozon: OzonClient,
    days_back: int,
    statuses: Iterable[str],
) -> List[dict]:
    by_number: Dict[str, dict] = {}
    for st in statuses:
        for p in iter_ozon_postings_last_ndays(ozon, days_back, status=st):
            num = str(p.get("posting_number") or "").strip()
            if num:
                by_number[num] = p
    return list(by_number.values())

def _parse_iso(dt_str: str):
    try:
        if isinstance(dt_str, str) and dt_str.endswith("Z"):
            return datetime.fromisoformat(dt_str.replace("Z", "+00:00"))
        return datetime.fromisoformat(dt_str)
    except Exception:
        return datetime.min.replace(tzinfo=timezone.utc)

# ====== –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ ======
def build_posting_fields(posting: dict) -> Dict[str, str]:
    return {
        'Posting Number': clean_value(posting.get('posting_number')),
        'In Process At': iso_to_moscow_str(posting.get('in_process_at')),
        'Shipment Date': iso_to_moscow_str(posting.get('shipment_date')),
        'Status': clean_value(posting.get('status')),
    }

def build_product_fields(product: Optional[dict]) -> Dict[str, str]:
    if not product:
        return {'Product Name': '', 'Offer ID': '', 'Product Quantity': ''}
    return {
        'Product Name': clean_value(product.get('name')),
        'Offer ID': clean_value(product.get('offer_id')),
        'Product Quantity': clean_value(product.get('quantity')),
    }

# --- –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø ACTIONS ---
ACTIONS_EXCLUDE = {
    "–û–∫—Ä—É–≥–ª–µ–Ω–∏–µ",
    "–¢–æ–≤–∞—Ä–Ω–∞—è —Å–∫–∏–¥–∫–∞ –Ω–∞ –¥–æ—Å—Ç–∞–≤–∫—É (–°–∫–≤–æ–∑–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏–∫–∞ 5)",
    "–°–∏—Å—Ç–µ–º–Ω–∞—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è —Å–∫–∏–¥–∫–∞ —Å–µ–ª–ª–µ—Ä–∞ –†–æ—Å—Å–∏—è (RUB)",
    "–ê–∫—Ü–∏—è –Ω–∞ —Å–ø–∏—Å–∞–Ω–∏–µ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –±–æ–Ω—É—Å–æ–≤ –¥–ª—è —Å–µ–ª–ª–µ—Ä–∞ 160517 –æ—Ç 11.10.23 13:18:34",
    "OA by AI benefit system (Mesh)", "–°–∫–∏–¥–∫–∞ –∑–∞ —Å—á–µ—Ç Ozon",
    "DD by AI benefit system (Mesh)", "–°–∫–∏–¥–∫–∞ –∑–∞ —Å—á–µ—Ç Ozon",
    "–°–∫–∏–¥–∫–∞ (–∑–∞ —Å—á–µ—Ç –û–∑–æ–Ω) - DD by AI benefit system (Mesh)",
    "–°–∫–∏–¥–∫–∞ (–∑–∞ —Å—á–µ—Ç –û–∑–æ–Ω) - OA by AI benefit system (Mesh)",
    "–°–∫–∏–¥–∫–∞ (–∑–∞ —Å—á–µ—Ç –û–∑–æ–Ω) - DD by AI benefit system (CIS-Benefit1) v2",
    "[–û–ø–ª–∞—Ç–∞ –ë–∞–ª–ª–∞–º–∏] –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —É—Å–ª–æ–≤–∏—è (–¥–æ 25%)",
}

ACTIONS_MAP_CONTAINS = [
    ("–ë—É—Å—Ç–∏–Ω–≥ 25% (—Ä–∞–Ω–µ–µ ‚Äî ¬´–ë—É—Å—Ç–∏–Ω–≥ —Ö4¬ª)", "–ë—É—Å—Ç–∏–Ω–≥ 25%"),
    ("–ë—É—Å—Ç–∏–Ω–≥ 15% (—Ä–∞–Ω–µ–µ ‚Äî ¬´–ë—É—Å—Ç–∏–Ω–≥ —Ö3¬ª)", "–ë—É—Å—Ç–∏–Ω–≥ 15%"),
    ("–≠–ª–∞—Å—Ç–∏—á–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥. –ë–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å—Ä–æ–∫–∞ –¥–µ–π—Å—Ç–≤–∏—è", "–≠–ª–∞—Å—Ç–∏—á–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥"),
    ("–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–ª–ª—ã –∑–∞ —Å–∫–∏–¥–∫–∏ –æ—Ç –û–∑–æ–Ω –∏ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥",
     "–î–æ–ø. –±–∞–ª–ª—ã –∑–∞ —Å–∫–∏–¥–∫–∏ –æ—Ç –û–∑–æ–Ω –∏ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥"),
]

_PAREN_RE = re.compile(r"\s*\(—Ä–∞–Ω–µ–µ\s+‚Äî\s+¬´[^¬ª]+¬ª\)\s*$", re.IGNORECASE)

def _normalize_single_action(a: str) -> Optional[str]:
    if a is None:
        return None
    t = str(a).strip()
    if not t:
        return None
    if t in ACTIONS_EXCLUDE:
        return None

    low = t.lower()
    for needle, repl in ACTIONS_MAP_CONTAINS:
        if needle.lower() in low:
            t = repl
            break

    t = _PAREN_RE.sub("", t).strip()
    return t or None

def _join_actions(a) -> str:
    if not a:
        return ''
    if isinstance(a, (list, tuple)):
        out, seen = [], set()
        for x in a:
            nx = _normalize_single_action(x)
            if not nx:
                continue
            if nx not in seen:
                seen.add(nx)
                out.append(nx)
        return ", ".join(out)
    nx = _normalize_single_action(a)
    return nx or ''

def _find_financial_for_product(financial_data: dict, product: Optional[dict]) -> dict:
    """
    –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å financial_data.products[*] –¥–ª—è —Ç–æ–≤–∞—Ä–∞:
    –º–∞—Ç—á –ø–æ SKU ‚Üî product_id, –∏–Ω–∞—á–µ –µ—Å–ª–∏ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å ‚Äî –±–µ—Ä—ë–º –µ—ë.
    """
    if not financial_data or not isinstance(financial_data, dict):
        return {}
    fin_products = financial_data.get('products') or []
    if not fin_products:
        return {}

    sku = None
    if product and isinstance(product, dict):
        sku = product.get('sku') or product.get('SKU')

    match = None
    if sku is not None:
        for fp in fin_products:
            try:
                if int(fp.get('product_id', -1)) == int(sku):
                    match = fp
                    break
            except Exception:
                continue

    if match is None and len(fin_products) == 1:
        match = fin_products[0]

    return match or {}

def build_financial_fields_per_product(fin_prod: dict, financial_data: dict) -> Dict[str, str]:
    cluster_to = (financial_data or {}).get('cluster_to')
    return {
        'Cluster To': clean_value(cluster_to),
        'Fin Price': clean_value(fin_prod.get('price')),
        'Fin Actions': _join_actions(fin_prod.get('actions')),
    }

def build_full_row_dict(posting: dict, product: Optional[dict]) -> Dict[str, str]:
    d = {}
    d.update(build_posting_fields(posting))
    d.update(build_product_fields(product))

    fin = posting.get('financial_data') or {}
    fin_prod = _find_financial_for_product(fin, product)
    d.update(build_financial_fields_per_product(fin_prod, fin))

    for k in FULL_HEADERS_EXTENDED:
        d.setdefault(k, '')
    return d

def row_to_final_order(d: Dict[str, str]) -> List[str]:
    return [d.get(h, '') for h in FULL_HEADERS_EXTENDED]

def process_postings(postings: Iterable[dict]) -> List[List[str]]:
    rows = []
    for p in postings:
        prods = p.get('products') or []
        if not prods:
            rows.append(row_to_final_order(build_full_row_dict(p, None)))
        else:
            for pr in prods:
                if isinstance(pr, dict):
                    rows.append(row_to_final_order(build_full_row_dict(p, pr)))
    return rows

# ====== Google Sheets ======
def _col_letter(n: int) -> str:
    s = ""
    while n > 0:
        n, r = divmod(n - 1, 26)
        s = chr(65 + r) + s
    return s

def get_sheet():
    client = get_gspread_client()
    spreadsheet = client.open_by_key(SPREADSHEET_ID)
    return spreadsheet.worksheet(SHEET_NAME)

def clear_header_cells(sheet):
    try:
        sheet.batch_clear(["A1:A3", "D3", "H3"])
        log.info("–Ø—á–µ–π–∫–∏ A1:A3, D3 –∏ H3 –æ—á–∏—â–µ–Ω—ã")
    except Exception as e:
        log.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ —è—á–µ–µ–∫: {e}")

def update_google_sheet_batch(sheet, headers: List[str], rows: List[List[str]]):
    last_col_letter = _col_letter(len(FULL_HEADERS_EXTENDED))
    last_row = HEADER_ROW if not rows else START_ROW + len(rows) - 1

    try:
        sheet.batch_clear([f"A{HEADER_ROW}:{last_col_letter}{max(last_row, START_ROW + 5000)}"])
    except Exception as e:
        log.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω: {e}")

    updates = [{"range": f"A{HEADER_ROW}:{last_col_letter}{HEADER_ROW}", "values": [headers]}]
    if rows:
        updates.append({"range": f"A{START_ROW}:{last_col_letter}{last_row}", "values": rows})
    sheet.batch_update(updates)

def update_first_last_shipment(sheet, rows: List[List[str]]):
    if not rows:
        log.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è D3/H3")
        return
    nums = [r[0] for r in rows if r and str(r[0]).strip()]
    if not nums:
        log.info("–ù–µ—Ç –Ω–æ–º–µ—Ä–æ–≤ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–π –¥–ª—è D3/H3")
        return
    sheet.batch_update([
        {"range": "D3", "values": [[nums[0]]]},
        {"range": "H3", "values": [[nums[-1]]]},
    ])
    log.info(f"–û–±–Ω–æ–≤–ª–µ–Ω—ã D3={nums[0]}, H3={nums[-1]}")

def update_header_info(sheet, total_postings: int):
    now = datetime.now(TZ_MSK)
    a1 = f"–û–±–Ω–æ–≤–ª–µ–Ω–æ {now:%d.%m|%H:%M}"
    a2 = f"–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–π {total_postings}"
    a3 = f"–≠—Ç–∏–∫–µ—Ç–æ–∫ —Å–æ–∑–¥–∞–Ω–æ 0"
    sheet.batch_update([
        {"range": "A1", "values": [[a1]]},
        {"range": "A2", "values": [[a2]]},
        {"range": "A3", "values": [[a3]]},
    ])
    log.info("A1/A2/A3 –æ–±–Ω–æ–≤–ª–µ–Ω—ã")

# ====== –û—Å–Ω–æ–≤–Ω–æ–π —Å—Ü–µ–Ω–∞—Ä–∏–π ======
def main():
    ozon = None
    try:
        sheet = get_sheet()

        log.info("–û—á–∏—Å—Ç–∫–∞ —Å–ª—É–∂–µ–±–Ω—ã—Ö —è—á–µ–µ–∫...")
        clear_header_cells(sheet)

        log.info("–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Ozon...")
        ozon = OzonClient(CLIENT_ID, API_KEY)

        # –í–ê–ñ–ù–û: awaiting_delivery –£–ë–†–ê–õ–ò ‚Äî –æ–Ω –Ω–µ–≤–∞–ª–∏–¥–µ–Ω –¥–ª—è –º–µ—Ç–æ–¥–∞ list
        statuses = ["awaiting_packaging", "awaiting_deliver"]
        postings = fetch_postings_multi_status(ozon, DAYS_BACK, statuses)

        postings.sort(key=lambda p: _parse_iso(p.get("in_process_at") or ""))

        total = len(postings)
        log.info(f"–ü–æ–ª—É—á–µ–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏–π: {total}")

        log.info("–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫...")
        rows = process_postings(postings)
        log.info(f"–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ —Å—Ç—Ä–æ–∫: {len(rows)}")

        log.info("–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Google-—Ç–∞–±–ª–∏—Ü—ã...")
        update_google_sheet_batch(sheet, make_final_headers(), rows)
        update_first_last_shipment(sheet, rows)

        log.info("–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤...")
        update_header_info(sheet, total_postings=total)

        log.info("–ì–æ—Ç–æ–≤–æ!")
    except Exception as e:
        log.error(f"–û—à–∏–±–∫–∞: {e}")
    finally:
        try:
            if ozon and ozon.s:
                ozon.s.close()
        except Exception:
            pass

if __name__ == "__main__":
    main()
