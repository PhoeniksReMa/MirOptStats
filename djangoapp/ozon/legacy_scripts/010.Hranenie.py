import argparse
import json
import os
import re
import sys
import time
import warnings
from datetime import datetime, date, timedelta
from urllib.parse import urlparse

import requests
import openpyxl

# --- suppress noisy openpyxl warning ---
warnings.filterwarnings(
    "ignore",
    message="Workbook contains no default style, apply openpyxl's default",
    category=UserWarning,
)

# ===== Google Sheets deps =====
try:
    from googleapiclient.discovery import build
    from google.oauth2.service_account import Credentials as SACredentials
    from google.oauth2.credentials import Credentials
    from google_auth_oauthlib.flow import InstalledAppFlow
    from google.auth.transport.requests import Request
except ImportError as e:
    raise ImportError(
        "–ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ Google API. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ:\n"
        "pip install --upgrade google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2"
    ) from e

SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]

# ====== OZON API ======
BASE_URL = "https://api-seller.ozon.ru"
CREATE_ENDPOINT = "/v1/report/placement/by-products/create"
INFO_ENDPOINT = "/v1/report/info"


# ====== Time helpers ======
def get_moscow_today_date() -> date:
    try:
        from zoneinfo import ZoneInfo  # Python 3.9+
        msk = ZoneInfo("Europe/Moscow")
        return datetime.now(msk).date()
    except Exception:
        return date.today()


# ====== Read config ======
def read_config(path: str = "API.txt") -> tuple[str, str, str, str]:
    """
    API.txt –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (4 —Å—Ç—Ä–æ–∫–∏):
    1) Client-ID (OZON)
    2) API-KEY   (OZON)
    3) ID Google —Ç–∞–±–ª–∏—Ü—ã
    4) –õ–∏—Å—Ç –∑–∞–ø–∏—Å–∏
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {path}")

    with open(path, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f.readlines() if line.strip()]

    if len(lines) < 4:
        raise ValueError(
            "–í API.txt –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 4 –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏:\n"
            "1) Client-Id\n2) Api-Key\n3) ID Google —Ç–∞–±–ª–∏—Ü—ã\n4) –ò–º—è –ª–∏—Å—Ç–∞ (–≤–∫–ª–∞–¥–∫–∏)"
        )

    return lines[0], lines[1], lines[2], lines[3]


# ====== OZON HTTP ======
def ozon_post(client_id: str, api_key: str, endpoint: str, payload: dict) -> dict:
    url = BASE_URL + endpoint
    headers = {
        "Client-Id": client_id,
        "Api-Key": api_key,
        "Content-Type": "application/json",
        "Accept": "application/json",
    }

    try:
        r = requests.post(url, headers=headers, json=payload, timeout=60)
    except requests.RequestException as e:
        raise RuntimeError(f"[OZON] –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {endpoint}: {e}")

    if not r.ok:
        raise RuntimeError(f"[OZON] HTTP {r.status_code} –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {endpoint}. –û—Ç–≤–µ—Ç: {r.text}")

    try:
        return r.json()
    except ValueError:
        raise RuntimeError(f"[OZON] –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å JSON –æ—Ç {endpoint}. –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞: {r.text}")


def create_placement_report(client_id: str, api_key: str, date_from: str, date_to: str) -> str:
    payload = {"date_from": date_from, "date_to": date_to}
    data = ozon_post(client_id, api_key, CREATE_ENDPOINT, payload)

    code = data.get("code")
    if not code:
        raise RuntimeError(f"[OZON] –í –æ—Ç–≤–µ—Ç–µ create –Ω–µ—Ç –ø–æ–ª—è 'code'. –û—Ç–≤–µ—Ç: {json.dumps(data, ensure_ascii=False)}")
    return code


def get_report_info(client_id: str, api_key: str, code: str) -> dict:
    payload = {"code": code}
    data = ozon_post(client_id, api_key, INFO_ENDPOINT, payload)

    result = data.get("result")
    if not result:
        raise RuntimeError(f"[OZON] –í –æ—Ç–≤–µ—Ç–µ info –Ω–µ—Ç –ø–æ–ª—è 'result'. –û—Ç–≤–µ—Ç: {json.dumps(data, ensure_ascii=False)}")
    return result


def wait_report_ready(
    client_id: str,
    api_key: str,
    code: str,
    poll_seconds: int = 5,
    max_wait_seconds: int = 600,
) -> dict:
    started = time.time()
    last_status = None

    while True:
        result = get_report_info(client_id, api_key, code)
        status = result.get("status")
        error = result.get("error", "")

        if status != last_status:
            if status == "processing":
                print("[OZON] –û—Ç—á—ë—Ç —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è...")
            elif status == "success":
                print("[OZON] –û—Ç—á—ë—Ç –≥–æ—Ç–æ–≤.")
            else:
                print(f"[OZON] –°—Ç–∞—Ç—É—Å: {status}")
            last_status = status

        if status == "success":
            if not result.get("file"):
                raise RuntimeError(
                    f"[OZON] –°—Ç–∞—Ç—É—Å success, –Ω–æ –Ω–µ—Ç —Å—Å—ã–ª–∫–∏ 'file'. –û—Ç–≤–µ—Ç: {json.dumps(result, ensure_ascii=False)}"
                )
            return result

        if status in ("failed", "error"):
            raise RuntimeError(f"[OZON] –û—Ç—á—ë—Ç –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π. status={status}, error={error}")

        if time.time() - started > max_wait_seconds:
            raise TimeoutError(
                f"[OZON] –ù–µ –¥–æ–∂–¥–∞–ª–∏—Å—å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –æ—Ç—á—ë—Ç–∞ –∑–∞ {max_wait_seconds} —Å–µ–∫—É–Ω–¥. –ü–æ—Å–ª–µ–¥–Ω–∏–π status={status}"
            )

        time.sleep(poll_seconds)


def download_file(url: str, out_path: str) -> None:
    try:
        r = requests.get(url, stream=True, timeout=120)
        r.raise_for_status()
    except requests.RequestException as e:
        raise RuntimeError(f"[FILE] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª –æ—Ç—á—ë—Ç–∞: {e}")

    os.makedirs(os.path.dirname(out_path) or ".", exist_ok=True)
    with open(out_path, "wb") as f:
        for chunk in r.iter_content(chunk_size=1024 * 256):
            if chunk:
                f.write(chunk)


def guess_extension_from_url(file_url: str) -> str:
    path = urlparse(file_url).path
    return os.path.splitext(path)[1].lower()


# ====== RAW XLSX aggregation ======
def _to_float(v) -> float:
    if v is None:
        return 0.0
    if isinstance(v, (int, float)):
        return float(v)
    s = str(v).strip().replace(" ", "").replace(",", ".")
    try:
        return float(s)
    except Exception:
        return 0.0


def _to_int(v) -> int:
    return int(round(_to_float(v)))


def _parse_date(v) -> date | None:
    if v is None:
        return None
    if isinstance(v, date) and not isinstance(v, datetime):
        return v
    if isinstance(v, datetime):
        return v.date()
    s = str(v).strip()
    for fmt in ("%Y-%m-%d", "%d.%m.%Y", "%d-%m-%Y", "%Y.%m.%d"):
        try:
            return datetime.strptime(s, fmt).date()
        except Exception:
            pass
    return None


def _find_date_column(headers: list[str]) -> str | None:
    if "–î–∞—Ç–∞" in headers:
        return "–î–∞—Ç–∞"
    candidates = [h for h in headers if "–î–∞—Ç–∞" in h]
    if not candidates:
        return None
    for h in candidates:
        if "–µ–∂–µ–¥–Ω–µ–≤" in h.lower():
            return h
    return candidates[0]


def aggregate_from_raw_xlsx(raw_xlsx_path: str, snapshot_date_to: str) -> list[dict]:
    """
    DK = cost_total –∑–∞ –ø–µ—Ä–∏–æ–¥ (—Ü–µ–ª–æ–µ)
    DL = qty_paid (–Ω–∞ –¥–∞—Ç—É —Å—Ä–µ–∑–∞)
    DM = forecast_28_days_rub = cost_on_snap_day * 28 (—Ü–µ–ª–æ–µ)
    DN = wh_count (–Ω–∞ –¥–∞—Ç—É —Å—Ä–µ–∑–∞)
    NOTE (—Ç–æ–ª—å–∫–æ DN) = "üîπ –°–ö–õ–ê–î - N —à—Ç" –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
    """
    if not os.path.exists(raw_xlsx_path):
        raise FileNotFoundError(f"[FILE] RAW —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {raw_xlsx_path}")

    target_date = datetime.fromisoformat(snapshot_date_to).date()

    wb = openpyxl.load_workbook(raw_xlsx_path, data_only=True, read_only=True)
    ws = wb.active

    header_row = next(ws.iter_rows(min_row=1, max_row=1, values_only=True))
    headers = [str(h).strip() if h is not None else "" for h in header_row]
    idx = {name: i for i, name in enumerate(headers)}

    date_col = _find_date_column(headers)
    if not date_col:
        raise RuntimeError("[XLSX] –ù–µ –Ω–∞—à—ë–ª –∫–æ–ª–æ–Ω–∫—É —Å –¥–∞—Ç–æ–π –≤ RAW XLSX.")

    required = [
        date_col,
        "–°–∫–ª–∞–¥",
        "SKU",
        "–ö–æ–ª-–≤–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤",
        "–ö–æ–ª-–≤–æ –ø–ª–∞—Ç–Ω—ã—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤",
        "–ù–∞—á–∏—Å–ª–µ–Ω–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å —Ä–∞–∑–º–µ—â–µ–Ω–∏—è",
    ]
    missing = [c for c in required if c not in idx]
    if missing:
        raise RuntimeError(
            "[XLSX] –ù–µ –Ω–∞—à—ë–ª –æ–∂–∏–¥–∞–µ–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ XLSX.\n"
            f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {missing}\n"
            f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏: {headers}"
        )

    cost_sum_by_sku: dict[int, float] = {}
    snap_by_sku_day: dict[int, dict[date, dict]] = {}

    for row in ws.iter_rows(min_row=2, values_only=True):
        d = _parse_date(row[idx[date_col]])
        if d is None:
            continue

        sku_raw = row[idx["SKU"]]
        sku = _to_int(sku_raw) if sku_raw is not None else 0
        if sku == 0:
            continue

        wh_raw = row[idx["–°–∫–ª–∞–¥"]]
        wh = str(wh_raw).strip() if wh_raw is not None else ""
        if not wh:
            wh = "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

        qty_total = _to_int(row[idx["–ö–æ–ª-–≤–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤"]])
        qty_paid = _to_int(row[idx["–ö–æ–ª-–≤–æ –ø–ª–∞—Ç–Ω—ã—Ö —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤"]])
        cost = _to_float(row[idx["–ù–∞—á–∏—Å–ª–µ–Ω–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å —Ä–∞–∑–º–µ—â–µ–Ω–∏—è"]])

        cost_sum_by_sku[sku] = cost_sum_by_sku.get(sku, 0.0) + cost

        if sku not in snap_by_sku_day:
            snap_by_sku_day[sku] = {}
        if d not in snap_by_sku_day[sku]:
            snap_by_sku_day[sku][d] = {
                "qty_paid_total": 0,
                "daily_cost_total": 0.0,
                "wh_qty_total": {},
                "warehouses_nonzero": set(),
            }

        s = snap_by_sku_day[sku][d]
        s["qty_paid_total"] += qty_paid
        s["daily_cost_total"] += cost

        s["wh_qty_total"][wh] = s["wh_qty_total"].get(wh, 0) + qty_total
        if qty_total > 0:
            s["warehouses_nonzero"].add(wh)

    all_skus = sorted(set(list(cost_sum_by_sku.keys()) + list(snap_by_sku_day.keys())))

    rows_for_gs: list[dict] = []
    for sku in all_skus:
        cost_total = float(cost_sum_by_sku.get(sku, 0.0))

        day_map = snap_by_sku_day.get(sku, {})
        if day_map:
            available_days = sorted(day_map.keys())
            snap_day = target_date if target_date in day_map else available_days[-1]
            snap = day_map[snap_day]

            dl_qty_paid = int(snap["qty_paid_total"])
            daily_cost = float(snap["daily_cost_total"])

            # DM: –ø—Ä–æ–≥–Ω–æ–∑ 28 –¥–Ω–µ–π (—Ü–µ–ª–æ–µ)
            forecast_28 = int(round(daily_cost * 28.0))

            wh_qty_total: dict[str, int] = snap["wh_qty_total"]
            wh_nonzero = sorted(set(snap["warehouses_nonzero"]))
            wh_count = len(wh_nonzero)

            note_lines = [f"üîπ {wh} - {int(wh_qty_total.get(wh, 0))} —à—Ç" for wh in wh_nonzero]
            note = "\n".join(note_lines)
        else:
            dl_qty_paid = 0
            forecast_28 = 0
            wh_count = 0
            note = ""

        rows_for_gs.append(
            {
                "sku": sku,
                "dk": int(round(cost_total)),   # —Ü–µ–ª–æ–µ
                "dl": int(dl_qty_paid),
                "dm": int(forecast_28),         # —Ü–µ–ª–æ–µ
                "dn": int(wh_count),
                "note": note,
            }
        )

    return rows_for_gs


# ===== Google Sheets helpers =====
def _col_to_number(col: str) -> int:
    col = col.strip().upper()
    n = 0
    for ch in col:
        if not ("A" <= ch <= "Z"):
            raise ValueError(f"Bad column: {col}")
        n = n * 26 + (ord(ch) - ord("A") + 1)
    return n


def _norm_sku(v) -> str:
    if v is None:
        return ""
    s = str(v).strip()
    if not s:
        return ""
    try:
        return str(int(float(s.replace(",", "."))))
    except Exception:
        pass
    return re.sub(r"\D+", "", s)


def _a1(sheet_name: str, a1_range: str) -> str:
    safe = sheet_name.replace("'", "''")
    return f"'{safe}'!{a1_range}"


def get_sheets_service(credentials_path: str = "credentials.json", token_path: str = "token.json"):
    if not os.path.exists(credentials_path):
        raise FileNotFoundError(f"[GS] –ù–µ –Ω–∞–π–¥–µ–Ω {credentials_path} —Ä—è–¥–æ–º —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º")

    with open(credentials_path, "r", encoding="utf-8") as f:
        cred_json = json.load(f)

    # service account
    if isinstance(cred_json, dict) and cred_json.get("type") == "service_account":
        creds = SACredentials.from_service_account_file(credentials_path, scopes=SCOPES)
        return build("sheets", "v4", credentials=creds)

    # oauth client
    creds = None
    if os.path.exists(token_path):
        creds = Credentials.from_authorized_user_file(token_path, SCOPES)

    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(credentials_path, SCOPES)
            creds = flow.run_local_server(port=0)
        with open(token_path, "w", encoding="utf-8") as token:
            token.write(creds.to_json())

    return build("sheets", "v4", credentials=creds)


def get_sheet_id_and_rowcount(service, spreadsheet_id: str, sheet_name: str) -> tuple[int, int]:
    meta = service.spreadsheets().get(
        spreadsheetId=spreadsheet_id,
        fields="sheets(properties(sheetId,title,gridProperties(rowCount)))",
    ).execute()

    for sh in meta.get("sheets", []):
        props = sh.get("properties", {})
        if props.get("title") == sheet_name:
            sheet_id = int(props.get("sheetId"))
            row_count = int((props.get("gridProperties") or {}).get("rowCount", 0))
            return sheet_id, row_count

    raise RuntimeError(f"[GS] –ù–µ –Ω–∞–π–¥–µ–Ω –ª–∏—Å—Ç (–≤–∫–ª–∞–¥–∫–∞) '{sheet_name}' –≤ —Ç–∞–±–ª–∏—Ü–µ {spreadsheet_id}")


def _build_row_index_by_sku_in_column_griddata(
    service,
    spreadsheet_id: str,
    sheet_name: str,
    sku_col_letter: str = "K",
    start_row: int = 5,
    max_rows_cap: int = 50000,
) -> dict[str, list[int]]:
    _sheet_id, row_count = get_sheet_id_and_rowcount(service, spreadsheet_id, sheet_name)
    if row_count <= 0:
        return {}

    end_row = min(row_count, max_rows_cap)
    if end_row < start_row:
        return {}

    rng = _a1(sheet_name, f"{sku_col_letter}{start_row}:{sku_col_letter}{end_row}")

    resp = service.spreadsheets().get(
        spreadsheetId=spreadsheet_id,
        ranges=[rng],
        includeGridData=True,
        fields="sheets(data(rowData(values(formattedValue))))",
    ).execute()

    sheets = resp.get("sheets", [])
    if not sheets:
        return {}

    data = (sheets[0].get("data") or [])
    if not data:
        return {}

    row_data = (data[0].get("rowData") or [])
    row_map: dict[str, list[int]] = {}

    for i, rd in enumerate(row_data):
        values = rd.get("values") or []
        cell = values[0] if values else {}
        sku_raw = cell.get("formattedValue", "")
        sku = _norm_sku(sku_raw)
        if sku:
            gs_row = start_row + i
            row_map.setdefault(sku, []).append(gs_row)

    return row_map


def clear_values_dk_dn(service, spreadsheet_id: str, sheet_name: str, start_row: int, end_row: int):
    """–û—á–∏—â–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –≤ DK, DL, DM, DN."""
    if end_row < start_row:
        return
    rng = _a1(sheet_name, f"DK{start_row}:DN{end_row}")
    service.spreadsheets().values().clear(
        spreadsheetId=spreadsheet_id,
        range=rng,
        body={}
    ).execute()


def clear_notes_in_dn(service, spreadsheet_id: str, sheet_id: int, start_row: int, end_row: int):
    """–û—á–∏—â–∞–µ—Ç –≤—Å–µ —Å—Ç–∞—Ä—ã–µ –ø—Ä–∏–º–µ—á–∞–Ω–∏—è (notes) —Ç–æ–ª—å–∫–æ –≤ DN."""
    if end_row < start_row:
        return
    dn_col = _col_to_number("DN")
    req = {
        "requests": [
            {
                "repeatCell": {
                    "range": {
                        "sheetId": sheet_id,
                        "startRowIndex": start_row - 1,
                        "endRowIndex": end_row,
                        "startColumnIndex": dn_col - 1,
                        "endColumnIndex": dn_col,
                    },
                    "cell": {"note": ""},
                    "fields": "note",
                }
            }
        ]
    }
    service.spreadsheets().batchUpdate(spreadsheetId=spreadsheet_id, body=req).execute()


def write_totals_values(service, spreadsheet_id: str, sheet_name: str, total_row: int, totals: list[int]):
    """
    totals = [sum_DK, sum_DL, sum_DM, sum_DN]
    –ü–∏—à–µ—Ç –≤ DK{total_row}:DN{total_row} —á–∏—Å–ª–∞ (–ù–ï —Ñ–æ—Ä–º—É–ª—ã).
    """
    rng = _a1(sheet_name, f"DK{total_row}:DN{total_row}")
    service.spreadsheets().values().update(
        spreadsheetId=spreadsheet_id,
        range=rng,
        valueInputOption="RAW",
        body={"values": [totals]},
    ).execute()


def push_to_google_sheet(
    rows_for_gs: list[dict],
    spreadsheet_id: str,
    sheet_name: str,
    credentials_path: str = "credentials.json",
    sku_col_letter: str = "K",
    start_row: int = 5,
    totals_row: int = 3,  # –∏—Ç–æ–≥–∏ –≤ —Å—Ç—Ä–æ–∫—É 3
):
    """
    –ü–µ—Ä–µ–¥ –∑–∞–ø–∏—Å—å—é:
      - –æ—á–∏—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è DK,DL,DM,DN
      - –æ—á–∏—â–∞–µ–º –ø—Ä–∏–º–µ—á–∞–Ω–∏—è DN

    –ó–∞—Ç–µ–º:
      - –ø–∏—à–µ–º —Ç–æ–ª—å–∫–æ –≥–¥–µ DL>0
      - –ø—Ä–∏–º–µ—á–∞–Ω–∏—è —Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ –≤ DN

    –ò—Ç–æ–≥–∏:
      - —Å—á–∏—Ç–∞–µ–º –≤ –ø–∞–º—è—Ç–∏ —Å—É–º–º—ã –ø–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø–∏—Å–∞–Ω–Ω—ã–º —Å—Ç—Ä–æ–∫–∞–º (—É—á–∏—Ç—ã–≤–∞—è –¥—É–±–ª–∏ SKU –≤ –∫–æ–ª–æ–Ω–∫–µ K)
      - –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ DK3..DN3 –∫–∞–∫ —á–∏—Å–ª–∞
    """
    service = get_sheets_service(credentials_path=credentials_path)
    sheet_id, row_count = get_sheet_id_and_rowcount(service, spreadsheet_id, sheet_name)

    print(f"[GS] –ß–∏—Å—Ç–∏–º –¥–∏–∞–ø–∞–∑–æ–Ω DK{start_row}:DN{row_count} –∏ –ø—Ä–∏–º–µ—á–∞–Ω–∏—è DN...")
    clear_values_dk_dn(service, spreadsheet_id, sheet_name, start_row, row_count)
    clear_notes_in_dn(service, spreadsheet_id, sheet_id, start_row, row_count)
    print(f"[GS] –û–∫: –æ—á–∏—â–µ–Ω–æ DK{start_row}:DN{row_count} + notes(DN)")

    row_map = _build_row_index_by_sku_in_column_griddata(
        service,
        spreadsheet_id,
        sheet_name,
        sku_col_letter=sku_col_letter,
        start_row=start_row,
    )

    dn_col_num = _col_to_number("DN")

    value_updates = []
    note_requests = []

    updated_rows = 0
    matched_skus = 0
    skipped_skus = 0
    notes_set = 0

    # SKU, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ Google Sheet (–≤ –∫–æ–ª–æ–Ω–∫–µ sku_col_letter)
    not_found_skus: list[str] = []

    # –ò—Ç–æ–≥–∏ –ø–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–º —Å—Ç—Ä–æ–∫–∞–º (–∞–Ω–∞–ª–æ–≥ SUM(DK5:DK) –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏, –ø—É—Å—Ç—ã–µ=0)
    totals_dk = 0
    totals_dl = 0
    totals_dm = 0
    totals_dn = 0

    def chunks(lst, n):
        for i in range(0, len(lst), n):
            yield lst[i:i + n]

    for r in rows_for_gs:
        sku_key = _norm_sku(r.get("sku"))
        if not sku_key:
            skipped_skus += 1
            continue

        gs_rows = row_map.get(sku_key)
        if not gs_rows:
            skipped_skus += 1
            not_found_skus.append(sku_key)
            continue

        matched_skus += 1

        dl = int(r.get("dl", 0))
        if dl == 0:
            continue  # –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –ø—É—Å—Ç–æ

        dk = int(r.get("dk", 0))
        dm = int(r.get("dm", 0))
        dn = int(r.get("dn", 0))
        note = (r.get("note") or "").strip()

        for gs_row in gs_rows:
            rng = _a1(sheet_name, f"DK{gs_row}:DN{gs_row}")
            value_updates.append({"range": rng, "values": [[dk, dl, dm, dn]]})
            updated_rows += 1

            totals_dk += dk
            totals_dl += dl
            totals_dm += dm
            totals_dn += dn

            if note:
                notes_set += 1
                note_requests.append(
                    {
                        "repeatCell": {
                            "range": {
                                "sheetId": sheet_id,
                                "startRowIndex": gs_row - 1,
                                "endRowIndex": gs_row,
                                "startColumnIndex": dn_col_num - 1,
                                "endColumnIndex": dn_col_num,
                            },
                            "cell": {"note": note},
                            "fields": "note",
                        }
                    }
                )

    # values
    for part in chunks(value_updates, 500):
        service.spreadsheets().values().batchUpdate(
            spreadsheetId=spreadsheet_id,
            body={"valueInputOption": "USER_ENTERED", "data": part},
        ).execute()

    # notes
    for part in chunks(note_requests, 500):
        service.spreadsheets().batchUpdate(
            spreadsheetId=spreadsheet_id,
            body={"requests": part},
        ).execute()

    # –ò—Ç–æ–≥–∏ –≤ —Å—Ç—Ä–æ–∫—É 3 (–∫–∞–∫ —á–∏—Å–ª–∞)
    write_totals_values(
        service,
        spreadsheet_id,
        sheet_name,
        total_row=totals_row,
        totals=[totals_dk, totals_dl, totals_dm, totals_dn],
    )

    # –∫—Ä–∞—Å–∏–≤–æ –ø–æ–∫–∞–∂–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ SKU
    not_found_skus = sorted(set(not_found_skus), key=lambda x: (len(x), x))
    if not_found_skus:
        preview = ", ".join(not_found_skus[:50])
        tail = "" if len(not_found_skus) <= 50 else f" ... (+{len(not_found_skus)-50})"
        not_found_msg = f"–ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤ Google Sheet (–∫–æ–ª–æ–Ω–∫–∞ {sku_col_letter}): {preview}{tail}"
    else:
        not_found_msg = "–ù–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö SKU –Ω–µ—Ç"

    print(
        f"[GS] –û–±–Ω–æ–≤–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫ (DK..DN, –≥–¥–µ DL>0): {updated_rows}\n"
        f"[GS] SKU —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ: {matched_skus}, –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {len(not_found_skus)}\n"
        f"[GS] –ü—Ä–∏–º–µ—á–∞–Ω–∏–π DN —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: {notes_set}\n"
        f"[GS] –ò—Ç–æ–≥–∏: DK{totals_row}={totals_dk}, DL{totals_row}={totals_dl}, DM{totals_row}={totals_dm}, DN{totals_row}={totals_dn}\n"
        f"[GS] {not_found_msg}"
    )


# ====== CLI / MAIN ======
def main():
    parser = argparse.ArgumentParser(
        description="ONLINE: OZON placement by products -> —Å–∫–∞—á–∞—Ç—å RAW.xlsx -> –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å -> –∑–∞–ø–∏—Å–∞—Ç—å –≤ Google Sheets (DK-DN) –ø–æ SKU –∏–∑ K5:K."
    )
    parser.add_argument("--api-file", default="API.txt",
                        help="API.txt (4 —Å—Ç—Ä–æ–∫–∏: Client-Id, Api-Key, spreadsheet_id, sheet_name)")
    parser.add_argument("--credentials", default="credentials.json", help="credentials.json –¥–ª—è Google")

    parser.add_argument("--days", type=int, default=28, help="–°–∫–æ–ª—å–∫–æ —Å—É—Ç–æ–∫ –º–µ–∂–¥—É date_from –∏ date_to (0..30)")
    parser.add_argument("--date-from", default=None, help="YYYY-MM-DD (–µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω–æ ‚Äî date_to - days)")
    parser.add_argument("--date-to", default=None, help="YYYY-MM-DD (–µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω–æ ‚Äî —Å–µ–≥–æ–¥–Ω—è –ø–æ –ú–æ—Å–∫–≤–µ)")

    parser.add_argument("--poll", type=int, default=5, help="–ò–Ω—Ç–µ—Ä–≤–∞–ª –æ–ø—Ä–æ—Å–∞ /v1/report/info (—Å–µ–∫)")
    parser.add_argument("--max-wait", type=int, default=600, help="–ú–∞–∫—Å. –æ–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –æ—Ç—á—ë—Ç–∞ (—Å–µ–∫)")
    parser.add_argument("--code", default=None, help="–ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å code ‚Äî –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å create –∏ —Å—Ä–∞–∑—É –∂–¥–∞—Ç—å/—Å–∫–∞—á–∞—Ç—å")

    parser.add_argument("--dir", default=".", help="–ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è RAW")
    parser.add_argument("--prefix", default=None, help="–ü—Ä–µ—Ñ–∏–∫—Å –∏–º–µ–Ω–∏ RAW —Ñ–∞–π–ª–∞")

    parser.add_argument("--gs-sku-col", default="K", help="–ö–æ–ª–æ–Ω–∫–∞ –≤ Google Sheet –≥–¥–µ SKU (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é K)")
    parser.add_argument("--gs-start-row", type=int, default=5, help="–°—Ç—Ä–æ–∫–∞, —Å –∫–æ—Ç–æ—Ä–æ–π –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è –¥–∞–Ω–Ω—ã–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)")

    args = parser.parse_args()

    if args.days < 0 or args.days > 30:
        raise ValueError("--days –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0..30 (–ø–µ—Ä–∏–æ–¥ <= 31 –¥–µ–Ω—å –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ).")

    client_id, api_key, spreadsheet_id, sheet_name = read_config(args.api_file)

    # dates
    date_to = args.date_to or get_moscow_today_date().isoformat()
    dt_to = datetime.fromisoformat(date_to).date()

    if args.date_from:
        date_from = args.date_from
    else:
        date_from = (dt_to - timedelta(days=args.days)).isoformat()

    dt_from = datetime.fromisoformat(date_from).date()
    if (dt_to - dt_from).days > 30:
        raise ValueError("–ü–µ—Ä–∏–æ–¥ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: (date_to - date_from).days –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å <= 30.")

    print(f"[OZON] –ü–µ—Ä–∏–æ–¥: {date_from} .. {date_to}")

    # create/wait
    if args.code:
        code = args.code
        print(f"[OZON] –ò—Å–ø–æ–ª—å–∑—É–µ–º code: {code}")
    else:
        code = create_placement_report(client_id, api_key, date_from, date_to)
        print(f"[OZON] –û—Ç—á—ë—Ç —Å–æ–∑–¥–∞–Ω: {code}")

    result = wait_report_ready(client_id, api_key, code, poll_seconds=args.poll, max_wait_seconds=args.max_wait)

    file_url = result["file"]

    # download RAW
    os.makedirs(args.dir, exist_ok=True)
    safe_code = code.replace("/", "_").replace("\\", "_")
    prefix = args.prefix or f"placement_by_products_{date_from}_to_{date_to}"
    ext = guess_extension_from_url(file_url) or ".xlsx"
    raw_path = os.path.join(args.dir, f"{prefix}_RAW_{safe_code}{ext}")

    download_file(file_url, raw_path)
    print(f"[FILE] RAW —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {raw_path}")

    if not raw_path.lower().endswith(".xlsx"):
        raise RuntimeError("[XLSX] Ozon –≤–µ—Ä–Ω—É–ª –Ω–µ XLSX ‚Äî –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–∑–º–æ–∂–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è .xlsx")

    # aggregate + push
    rows_for_gs = aggregate_from_raw_xlsx(raw_path, snapshot_date_to=date_to)

    print(f"[GS] –ó–∞–ø–∏—Å—å –≤ —Ç–∞–±–ª–∏—Ü—É: {spreadsheet_id} / –ª–∏—Å—Ç: '{sheet_name}'")
    push_to_google_sheet(
        rows_for_gs=rows_for_gs,
        spreadsheet_id=spreadsheet_id,
        sheet_name=sheet_name,
        credentials_path=args.credentials,
        sku_col_letter=args.gs_sku_col,
        start_row=args.gs_start_row,
        totals_row=3,
    )


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"[error] {e}", file=sys.stderr)
        sys.exit(1)
